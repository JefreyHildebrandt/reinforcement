{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart pole #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "def compute_modified_reward(next_state):\n",
    "    modified_reward = np.square(max(0, next_state[0] + 0.5))\n",
    "    if next_state[0] >= 0.5: \n",
    "        modified_reward += 1.0\n",
    "    return modified_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network has to be complex enough. Batch size has to be large enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "state_size = 2\n",
    "action_size = 3\n",
    "\n",
    "# Training parameters\n",
    "train_episodes = 501          # number of episodes to train\n",
    "max_steps = 200                # max number of step in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon_start = 1.0            # exploration probability at start\n",
    "epsilon_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.01            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q neural network parameters\n",
    "first_layer_size = 64         # number of neurons in first hidden layer\n",
    "second_layer_size = 64         # number of neurons in second hidden layer\n",
    "learning_rate = 0.0001         # learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 20000             # memory capacity\n",
    "batch_size = 100                # experience mini-batch size\n",
    "pretrain_length = memory_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        random_index_list = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size = batch_size, \n",
    "                               replace = False)\n",
    "        return [self.buffer[index] for index in random_index_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepopulate the experience memory ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty queue\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(pretrain_length):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, _, done, _ = env.step(action)\n",
    "    modified_reward = compute_modified_reward(next_state)\n",
    "    \n",
    "    memory.add((state, action, modified_reward, next_state, done))\n",
    "    \n",
    "    if done:\n",
    "        # Start new episode\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        # Go to next state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first hidden layer 128 neurons\n",
    "# second hidden layer 64 neurons\n",
    "\n",
    "class QNN:\n",
    "    def __init__(self, \n",
    "                 state_size = state_size, \n",
    "                 action_size = action_size, \n",
    "                 first_layer_size = 128,\n",
    "                 second_layer_size = 64,\n",
    "                 learning_rate = 0.01, \n",
    "                 name = 'qnn'):\n",
    "        \n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "    \n",
    "            # Prediction part of the QNN\n",
    "            # computes q(s, a) for given state s and all actions a\n",
    "            # so best action can be determined as argmax_a q(s, a)\n",
    "    \n",
    "            # Input layer, state s is input\n",
    "            self.state = tf.placeholder(tf.float32, \n",
    "                                        [None, state_size], \n",
    "                                        name = 'state')\n",
    "            \n",
    "            # First hidden layer, ReLU activation\n",
    "            self.first_layer = tf.contrib.layers.fully_connected(self.state, \n",
    "                                                                 first_layer_size)\n",
    "            # Second hidden layer, ReLU activation\n",
    "            self.second_layer = tf.contrib.layers.fully_connected(self.first_layer, \n",
    "                                                                  second_layer_size)\n",
    "\n",
    "            # Output layer, linear activation, q_vector(s, a) is output\n",
    "            self.q_vector = tf.contrib.layers.fully_connected(self.second_layer,\n",
    "                                                              action_size,\n",
    "                                                              activation_fn = None)\n",
    "    \n",
    "            # Training part of the Q-network\n",
    "            # uses observed transition (s, a, r, s') to update the weights of the network\n",
    "    \n",
    "            # Action a\n",
    "            self.action = tf.placeholder(tf.int32, \n",
    "                                         [None], \n",
    "                                         name = 'action')\n",
    "            # One-hot encoded action a \n",
    "            #\n",
    "            # encoded_action_vector = [1, 0] if action a = 0\n",
    "            # encoded_action_vector = [0, 1] if action a = 1\n",
    "            encoded_action_vector = tf.one_hot(self.action, \n",
    "                                               action_size)\n",
    "    \n",
    "            # Target Q value for training\n",
    "            # target_q_value = r + gamma * max_a' q(s', a') if state s' is non-terminal\n",
    "            # target_q_value = r                            if state s' is terminal\n",
    "            self.target_q_value = tf.placeholder(tf.float32, \n",
    "                                                 [None], \n",
    "                                                 name = 'target_q_value')\n",
    "    \n",
    "            # Q value for training\n",
    "            # q_value = q(s, a)\n",
    "            # It is important that everything is computed using linear-algebraic operations\n",
    "            # to leverage vectorization etc.\n",
    "            q_value = tf.reduce_sum(tf.multiply(self.q_vector, encoded_action_vector), \n",
    "                                         axis = 1)\n",
    "            # Loss\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_q_value - q_value))\n",
    "            \n",
    "            # Optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            \n",
    "    # get best action \n",
    "    def get_action(self, state):\n",
    "        feed_dict = { self.state : np.array([state]) } \n",
    "        q_vector = sess.run(self.q_vector, \n",
    "                            feed_dict = feed_dict)\n",
    "        return np.argmax(q_vector)\n",
    "        \n",
    "    # train based on a batch of data from \n",
    "    def update(self):    \n",
    "        \n",
    "        # Sample mini-batch from memory\n",
    "        batch = memory.sample(batch_size)\n",
    "        # some Python magic\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "    \n",
    "        state = np.array(state)\n",
    "        action = np.array(action)\n",
    "        reward = np.array(reward)\n",
    "        next_state = np.array(next_state)\n",
    "        mask = 1.0 - np.array(done)\n",
    "    \n",
    "        next_q_vector = sess.run(self.q_vector, \n",
    "                                 feed_dict = { \n",
    "                                     self.state : next_state \n",
    "                                 })\n",
    "        \n",
    "        target_q_value = reward + gamma * mask * np.max(next_q_vector, axis = 1)\n",
    "\n",
    "        sess.run([self.loss, self.optimizer],\n",
    "                 feed_dict = {\n",
    "                               self.state          : state,\n",
    "                               self.target_q_value : target_q_value,\n",
    "                               self.action         : action\n",
    "                 })\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning training algorithm ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: describe training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "qnn = QNN(name = 'qnn', \n",
    "          first_layer_size = first_layer_size,\n",
    "          second_layer_size = second_layer_size,\n",
    "          learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -200.0 Epsilon: 1.0000\n",
      "Displaying performance after episode: 0\n",
      "Total reward: -200.0\n",
      "Max position: -0.4787406853130801\n"
     ]
    }
   ],
   "source": [
    "# saver = tf.train.Saver()\n",
    "\n",
    "episode_reward_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #step = 0\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for episode in range(train_episodes):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Decrease epsilon\n",
    "        epsilon = epsilon_stop + (epsilon_start - epsilon_stop) * np.exp(-decay_rate * episode) \n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            #print(\"episode, step in episode:\", episode, s)\n",
    "            # Explore or exploit\n",
    "            if epsilon > np.random.rand():\n",
    "                # Select random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Compute q_vector using q neural network\n",
    "                action = qnn.get_action(state)\n",
    "\n",
    "            \n",
    "            \n",
    "            #print(step, epsilon)\n",
    "            \n",
    "            # Take epsilon-greedily selected action,\n",
    "            # obtain next state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            modified_reward = compute_modified_reward(next_state)\n",
    "            \n",
    "            # Add transition to memory\n",
    "            memory.add((state, action, modified_reward, next_state, done))\n",
    "\n",
    "            #step += 1\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Update weights of q neural network\n",
    "            qnn.update()\n",
    "            \n",
    "            if done: \n",
    "                #print(\"done!\")\n",
    "                break\n",
    "        \n",
    "        # Episode ended because either done or max_steps reached\n",
    "        \n",
    "        # Why does this code get executed twice????\n",
    "        \n",
    "        episode_reward_list.append((episode, total_reward))\n",
    "        \n",
    "        #print(\"Episode just ended:\", episode)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print('Episode: {}'.format(episode),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Epsilon: {:.4f}'.format(epsilon))\n",
    "        \n",
    "            state = env.reset()\n",
    "            max_pos = next_state[0]\n",
    "            total_reward = 0\n",
    "            print(\"Displaying performance after episode:\", episode)\n",
    "            for step in range(max_steps):\n",
    "                env.render()\n",
    "                time.sleep(0.1)\n",
    "                next_state, reward, done, _ = env.step(qnn.get_action(state))\n",
    "                total_reward += reward\n",
    "                if next_state[0] > max_pos:\n",
    "                    max_pos = next_state[0]\n",
    "                if done: \n",
    "                    print(\"Total reward:\", total_reward)\n",
    "                    print(\"Max position:\", max_pos)\n",
    "                    break\n",
    "                state = next_state\n",
    "            env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, rews = np.array(episode_reward_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha = 0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
