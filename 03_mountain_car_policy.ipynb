{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain car cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution for this game largely reuses the code from the cart_pole_cross_entropy_method from the examples.  The key differences is that the rewards had to be redone for the mountain car game.  Since the game itself doesn't actually give rewards, it just takes away awards the longer it takes to complete.\n",
    "\n",
    "The method I used gave the state of the current action as well as the direction as input parameters.  The reward initially was based off of velocity and the max height that was received in both directions.  Once the game completed, it gave a large reward which increased as the game completed faster.\n",
    "\n",
    "It took about 5 episodes for this implementation to start winning, and a few hundred to consistently complete in around 104 turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# Environment parameters\n",
    "state_size = 3\n",
    "action_size = 3\n",
    "\n",
    "hidden_layer_size = 128\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "max_episodes = 100\n",
    "\n",
    "max_steps = 200\n",
    "percentile = 70\n",
    "\n",
    "class Net:\n",
    "    def __init__(self, \n",
    "                 state_size = state_size, \n",
    "                 action_size = action_size, \n",
    "                 hidden_layer_size = hidden_layer_size,\n",
    "                 learning_rate = learning_rate, \n",
    "                 name = 'net'):\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "        \n",
    "            ### Prediction part\n",
    "        \n",
    "            # Input layer, state s is input\n",
    "            self.states = tf.placeholder(\n",
    "                tf.float32, \n",
    "                [None, state_size])\n",
    "            \n",
    "            # Hidden layer, ReLU activation\n",
    "            self.hidden_layer = tf.contrib.layers.fully_connected(\n",
    "                self.states, \n",
    "                hidden_layer_size)\n",
    "            \n",
    "            # Hidden layer, linear activation, logits\n",
    "            self.logits = tf.contrib.layers.fully_connected(\n",
    "                self.hidden_layer, \n",
    "                action_size,\n",
    "                activation_fn = None)\n",
    "            \n",
    "            # Output layer, softmax activation yields probability distribution for actions\n",
    "            self.probabilities = tf.nn.softmax(self.logits)\n",
    "    \n",
    "            ### Training part \n",
    "    \n",
    "            # Action a\n",
    "            self.actions = tf.placeholder(\n",
    "                tf.int32, \n",
    "                [None])\n",
    "            \n",
    "            # One-hot encoded action a \n",
    "            #\n",
    "            # encoded_action_vector = [1, 0] if action a = 0\n",
    "            # encoded_action_vector = [0, 1] if action a = 1\n",
    "            self.one_hot_actions = tf.one_hot(\n",
    "                self.actions, \n",
    "                action_size)\n",
    "\n",
    "            # cross entropy\n",
    "            self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits = self.logits, \n",
    "                labels = self.one_hot_actions)\n",
    "            \n",
    "            # cost\n",
    "            self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "            \n",
    "            # Optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "            \n",
    "    # get action chosen according to current probabilistic policy\n",
    "    def get_action(self, state):\n",
    "        feed_dict = { self.states : np.array([state]) } \n",
    "        probabilities = sess.run(self.probabilities, feed_dict = feed_dict)\n",
    "        return np.random.choice(action_size, p=probabilities[0])\n",
    "    \n",
    "    # train based on batch\n",
    "    def train(self, batch):\n",
    "        states, actions = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.states : states,\n",
    "            self.actions : actions,\n",
    "\n",
    "        }\n",
    "        \n",
    "        sess.run(self.optimizer, feed_dict = feed_dict)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "net = Net(name = 'net',\n",
    "          hidden_layer_size = hidden_layer_size,\n",
    "          learning_rate = learning_rate)\n",
    "\n",
    "import random\n",
    "import bisect\n",
    "import time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start_index = int(max_episodes * percentile / 100)\n",
    "    count = 0\n",
    "    is_passing = False\n",
    "    while True:\n",
    "        count += 1\n",
    "        total_reward_list = []\n",
    "        trajectory_list = []\n",
    "\n",
    "        for e in np.arange(max_episodes):\n",
    "            total_reward = 0.0\n",
    "            trajectory = []\n",
    "            state = env.reset()\n",
    "            # added additional context to the state where it knows if it was going forwards, backwards, or stayed the same relative to the previous state\n",
    "            state = (state[0], state[1], 0)\n",
    "            init_state = state\n",
    "            total_reward = 0\n",
    "            total_velocity = 0\n",
    "            reward = 0\n",
    "            highest_pos = -100\n",
    "            lowest_pos = 100\n",
    "            for s in np.arange(max_steps):\n",
    "                action = net.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                reward = next_state[0]\n",
    "                # give velocity high priority\n",
    "                velocity = abs(next_state[1] * 3)\n",
    "                total_velocity += velocity\n",
    "\n",
    "                # judge position only by the max height it received\n",
    "                if next_state[0] > highest_pos:\n",
    "                    highest_pos = next_state[0]\n",
    "                if next_state[0] < lowest_pos:\n",
    "                    lowest_pos = next_state[0]\n",
    "\n",
    "                # give additional positional context\n",
    "                forward = 0\n",
    "                if next_state[1] > state[1] and next_state[1]>0 and state[1]>0:\n",
    "                    forward = 1\n",
    "                elif next_state[1] < state[1] and next_state[1]<=0 and state[1]<=0:\n",
    "                    forward = -1\n",
    "\n",
    "                trajectory.append(((state[0], state[1], forward), action))\n",
    "                state = (next_state[0], next_state[1], forward)\n",
    "\n",
    "                # give a high reward based on how fast it was cleared\n",
    "                if done and s < 199: \n",
    "                    reward += 1000 * (200-s)\n",
    "                    break\n",
    "            total_reward = (abs(lowest_pos) + abs(highest_pos)) * total_velocity + reward\n",
    "            index = bisect.bisect(total_reward_list, total_reward)\n",
    "            total_reward_list.insert(index, total_reward)\n",
    "            trajectory_list.insert(index, trajectory)\n",
    "        \n",
    "        # keep the elite episodes, that is, throw out the bad ones \n",
    "        # train on state action pairs extracted from the elite episodes\n",
    "        # this code is not optimized, it can be cleaned up \n",
    "        state_action_pairs = []\n",
    "        for trajectory in trajectory_list[start_index:]:\n",
    "            for state_action_pair in trajectory:\n",
    "                state_action_pairs.append(state_action_pair)\n",
    "        # shuffle to avoid correlations between adjacent states\n",
    "        random.shuffle(state_action_pairs) \n",
    "        n = len(state_action_pairs)\n",
    "        batches = [state_action_pairs[k:k + batch_size] for k in np.arange(0, n, batch_size)]\n",
    "\n",
    "        for batch in batches:\n",
    "            net.train(batch)\n",
    "\n",
    "        # test agent\n",
    "        state = env.reset()\n",
    "        state = (state[0], state[1], 0)\n",
    "        env.render()\n",
    "        total_reward = 0.0\n",
    "        highest_state = 0\n",
    "        total_velocity = 0\n",
    "        highest_pos = -100\n",
    "        lowest_pos = 100\n",
    "        for s in np.arange(max_steps):\n",
    "            action = net.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            velocity = abs(next_state[1] * 3)\n",
    "            total_velocity += velocity\n",
    "            total_reward += reward\n",
    "            if next_state[0] > highest_pos:\n",
    "                highest_pos = next_state[0]\n",
    "            if next_state[0] < lowest_pos:\n",
    "                lowest_pos = next_state[0]\n",
    "            forward = 0\n",
    "            if next_state[1] > state[1] and next_state[1]>0 and state[1]>0:\n",
    "                forward = 1\n",
    "            elif next_state[1] < state[1] and next_state[1]<=0 and state[1]<=0:\n",
    "                forward = -1\n",
    "            env.render()\n",
    "            state = (next_state[0], next_state[1], forward)\n",
    "            if done: break\n",
    "\n",
    "        env.close()\n",
    "        h_state = (abs(lowest_pos) + abs(highest_pos)) * total_velocity\n",
    "        print(\"Total reward:\", total_reward, 'Count:', count, 'Highest State:', h_state)\n",
    "        \n",
    "        if total_reward > -200:\n",
    "            is_passing = True\n",
    "            print(\"Reached\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
